{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ewRfFs307zB"
   },
   "source": [
    "# Equivariant Neural Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dak5naPA3UPI"
   },
   "outputs": [],
   "source": [
    "import random, os, sys\n",
    "import cv2\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import image as mpimg\n",
    "%matplotlib inline\n",
    "sns.set() # Setting seaborn as default style even if use only matplotlib\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "import imageio\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\"\"\" Detect local path \"\"\"\n",
    "local_path = !pwd\n",
    "local_path = local_path[0]\n",
    "main_path = local_path[:-5]\n",
    "\n",
    "\n",
    "sys.path.append(local_path + '/../src/')\n",
    "sys.path.append(local_path + '/../src/enr/')\n",
    "from models.neural_renderer import *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBstX2KVztat"
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "\n",
    "The paper from Dupont et al. introduces an approach to render 2D images into implicit, equivariant 3D representations. The authors argue that the scene representations need not be explicit, as long as the transformations to it occur in an equivariant manner. Their model is trained on a dataset of rotation symmetries, learning to produce novel views from a single image of a scene.\n",
    "\n",
    "Current approaches in scene representations present difficulties with scalability. Voxel grids, point clouds and other traditional methods have high computational and memory requirements. Reconstrucion from incomplete or noisy data is also a challenging task with these methods, often requiring 3D information during training. Generating novel views of a scene given some input views presents the same difficulties. Finally, traditional neural networks are not equivariant with respect to general transformation groups. 3D equivariance especially requires specifc techniques like steerable filters. The authors attempt to solve these problems by proposing a new method which results in more scalable, implicit representations that are also equivariant with respect to transformations. \n",
    "\n",
    "The difference between an explicit scene representation (mesh grid) and an implicit one can be seen in the figure below:\n",
    "\n",
    "![Alt text](../src/imgs/paper_screenshots/fig2.png)\n",
    "\n",
    "  \n",
    "### 1.1: Methodology\n",
    "\n",
    "The proposed model uses a series of convolutions to map scene representations to images. Specifically, the scene representation is passed through 3D convolutions, followed by 1x1 convolutions and a set of 2D convolutions that maps them to image space. The reverse renderer is the transpose of this operation. Equivariance is enforced by applying transformations in both the scene and image spaces. Because the representation space is a deep voxel grid, the transformations in this space are defined by a 3D rotation matrix. Inverse warping with trilinear interpolation is also used in the model, to reconstruct the values after rotations.\n",
    "\n",
    "![Alt text](../src/imgs/paper_screenshots/fig5.png)\n",
    "\n",
    "For training, two images of the same scene are passed through the renderer. Then, the 3D transformation grid is applied in the representation space to turn one into the other, before passing them to the inverse renderer. Finally, the output images are compared to the original inputs to obtain the loss values. Training in this manner ensures the model learns equivariant representations, as the loss evaluates both the actual rendering and the accuracy of the matching transformations from both spaces.\n",
    "\n",
    "![Alt text](../src/imgs/paper_screenshots/fig4.png)\n",
    "\n",
    "Finally, the authors claim that the rendering loss used makes little change in results. They provide l1 norm, l2 norm and SSIM loss as candidates, and conduct ablation studies to determine the tradeoffs between them.\n",
    "\n",
    "### 1.2: Datasets\n",
    "The authors evaluate their model on 4 datasets, including two ShapeNet benchmarks as well as two novel datasets of the authors design. They use an image size of 128 x 128 and a representation size of 64 x 32 x 32 x 32.\n",
    "The datasets are presented in table 1. \n",
    "\n",
    "\n",
    "| *Dataset*  | *Source*  |  *Sample* | *# Scenes*  |*# images per scene*| *# datapoints*|\n",
    "|---|---|---|---|---||\n",
    "| Chairs  | [ShapeNet](https://icml20-prod.cdn-apple.com/eqn-data/data/chairs.zip)  | ![Chair](../src/imgs/paper_screenshots/chair.png)  |  6591 | 50  | 329 550|\n",
    "| Cars  |  [ShapeNet](https://icml20-prod.cdn-apple.com/eqn-data/data/cars.zip) | ![Car](../src/imgs/paper_screenshots/car.png)  |  3514 |  50 | 175 700|\n",
    "| MugsHQ  |  [Apple](https://icml20-prod.cdn-apple.com/eqn-data/data/mugs.zip) | ![Mug](../src/imgs/paper_screenshots/mug.png)  |  214 | 150  | 32 100|\n",
    "| 3D mountainset  |  [Apple](https://icml20-prod.cdn-apple.com/eqn-data/data/mountains.zip) | ![Mountain](../src/imgs/paper_screenshots/mountain.png)  |  559 |  50 | 27 950|\n",
    "\n",
    "Table 1.: *Overview of datasets considered for equivariant neural rendering by Dupont et al.*\n",
    "\n",
    "### 1.3: Experiments of paper\n",
    "\n",
    "The proposed model is compared against three baseline models, each one making assumptions much stronger than the original study.\n",
    "\n",
    "|   | TCO  |  DGQN | SRN  | Proposed model  |\n",
    "|---|---|---|---|---|\n",
    "| Requires Absolute Pose  | Yes  | Yes | Yes | No |\n",
    "| Requires Pose at Inference Time  | No  | Yes | Yes | No |\n",
    "| Optimization at Inference Time  | No  | No | Yes | No |\n",
    "\n",
    "The qualitative comparisons against the baseline models in single shot novel view synthesis with the ShapeNet chairs dataset reveals that the model achieves similar to SoTA results while making far fewer assumptions than the other methods. It can produce high quality novel views by achieving the desired equivariant transformation in representation space.\n",
    "\n",
    "![Alt text](../src/imgs/paper_screenshots/results.png)\n",
    "\n",
    "Results similar to the chairs were reported in the other datasets, with some variations due to the specific challenges of each one. For example, the mountains contain extremely complex geometric information, which severly limits the detail of the novel view synthesis.\n",
    "\n",
    "![Alt text](../src/imgs/paper_screenshots/chairs.png) \n",
    "\n",
    "![Alt text](../src/imgs/paper_screenshots/cars.png) \n",
    "\n",
    "![Alt text](../src/imgs/paper_screenshots/mugs.png) \n",
    "\n",
    "![Alt text](../src/imgs/paper_screenshots/mountains.png)\n",
    "\n",
    "Finally, the authors performed ablation studies to test novel view synthesis when using different loss functions. The results in each one were similar and no inherent prefered approach was suggested. In the end, they reason that choice of loss function is task specific.\n",
    "\n",
    "\n",
    "### 1.4 Demo of Original work \n",
    "\n",
    "The subsequent section presents a demonstration of the model capabilities of the model produced by Dupont et al. You may adjust the transformation parameters in order to transform the representation and render an input image from a new view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configure Transformations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Configure transformations for demonstration\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def transformation_dict(azimuth = 0., elevation = 0., x = 0., y = 0., z = 0.):\n",
    "    \"\"\" Store transformations in dictionary\n",
    "    \n",
    "    Args:\n",
    "        azimuth (angle): Positive (negative) values correspond to moving camera to the right (left)\n",
    "        Elevation (angle): Positive (negative) values correspond to moving camera up (down)\n",
    "        x, y, z (scalar units): translations along x, y, z axis \n",
    "    \"\"\"\n",
    "    transformations = {}\n",
    "    \n",
    "    # Rotation values\n",
    "    transformations['azimuth'] = torch.Tensor([azimuth]).to(device)\n",
    "    transformations['elevation'] = torch.Tensor([elevation]).to(device)\n",
    "\n",
    "    # Translation values\n",
    "    transformations['xyz'] = torch.Tensor([x, y, z]).to(device)\n",
    "\n",
    "    print('Transformations: ', transformations)\n",
    "    return transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this cell you can configure the transformations you wish \n",
    "to apply to the rendered image. You can set these to any value you like!\n",
    "\"\"\"\n",
    "\n",
    "# Transformed view\n",
    "transformations = transformation_dict(azimuth = 30., elevation = 30., \n",
    "                                      x = 0., y = -.2, z = .2)\n",
    "\n",
    "# initial view\n",
    "init_camera_pos = transformation_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define some helper functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_image(img, nrow=4):\n",
    "    \"\"\"Helper function to plot image tensors.\n",
    "    \n",
    "    Args:\n",
    "        img (torch.Tensor): Image or batch of images of shape \n",
    "            (batch_size, channels, height, width).\n",
    "    \"\"\"\n",
    "    \n",
    "    img_grid = torchvision.utils.make_grid(img, nrow=nrow)\n",
    "    return img_grid.cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "def render_image(model_path, img_path, downsample = False):\n",
    "    # ---------------------------------------------------------\n",
    "    # Loading the original image\n",
    "    # ---------------------------------------------------------\n",
    "    # Load trained chairs modellocal_path\n",
    "    model = load_model(model_path).to(device)\n",
    "\n",
    "    # You can also try loading other examples (e.g. 'chair1.png')\n",
    "    original_img = plt.imread(img_path)\n",
    "    \n",
    "    if downsample:\n",
    "        # Convert image to tensor and add batch dimension\n",
    "        original_img = cv2.resize(original_img, dsize=(64, 64), interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "    img_source = ToTensor()(original_img)\n",
    "    img_source = img_source.unsqueeze(0).to(device)\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # Render original image without transformations\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    # Infer scene representation\n",
    "    scene = model.inverse_render(img_source)\n",
    "\n",
    "    # We can render the scene representation without transforming it\n",
    "    rendered = model.render(scene)\n",
    "    rendered_img = tensor_to_image(rendered.detach())\n",
    "    \n",
    "    return scene, model, original_img, rendered_img\n",
    "\n",
    "def transform_scene(model,scene, init_camera_pos, transformations):\n",
    "    \"\"\"Helper function to transform scene\n",
    "    \n",
    "    Args: \n",
    "        model: model used to render scene\n",
    "        scene: implicit scene representation\n",
    "        transformations (dict): dict comprised of keys\n",
    "            azimuth (angle): Positive (negative) values correspond to moving camera to the right (left)\n",
    "            elevation (angle): Positive (negative) values correspond to moving camera up (down)\n",
    "            Translations (dict): translation values with dictionary elements corresponding to 'x', 'y', 'z'\n",
    "            \n",
    "     As a rotation matrix can feel a little abstract, we can also reason in terms of \n",
    "         camera azimuth and elevation. The initial coordinate at which the source image\n",
    "         is observed is given by the following azimuth and elevation. Note that these\n",
    "         are not necessary to generate novel views (as shown above), we just use them \n",
    "         for convenience to generate rotation matrices\n",
    "    \"\"\"\n",
    "    # Set Transformations and Configure output view\n",
    "    azimuth_target = init_camera_pos['azimuth'] + transformations['azimuth']\n",
    "    elevation_target = init_camera_pos['elevation'] + transformations['elevation']\n",
    "    translations_target = init_camera_pos['xyz'] + transformations['xyz']\n",
    "\n",
    "    # Rotate scene to match target camera angle\n",
    "    transformed_scene = model.rotate_source_to_target(\n",
    "        scene, \n",
    "        init_camera_pos['azimuth'], init_camera_pos['elevation'], init_camera_pos['xyz'],\n",
    "        azimuth_target, elevation_target, translations_target)\n",
    "    \n",
    "    # Render rotated scene\n",
    "    rendered = model.render(transformed_scene)\n",
    "    image = tensor_to_image(rendered.detach())\n",
    "    \n",
    "    return image\n",
    "\n",
    "def display_transformations(imgs, keys, fsize = (8, 6)):\n",
    "    \"\"\" Helper function to make suplot displaying images\n",
    "    \n",
    "    Args:\n",
    "        imgs (list): list of images to display\n",
    "        keys (list): list of plot titles\n",
    "        fsize (2-touple): figure size\n",
    "    \"\"\"\n",
    "    N = len(imgs)\n",
    "    fig, axs = plt.subplots(1, N, figsize=fsize)\n",
    "    for ax, idx in zip(axs, range(N)):\n",
    "        ax.imshow(imgs[idx])\n",
    "        ax.title.set_text(keys[idx])\n",
    "        \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLsHkFYj5s3O"
   },
   "source": [
    "**Loading, plotting & transforming the original image**\n",
    "\n",
    "The model infers from a single image and renders a second image from a novel view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Loading the original image\n",
    "# ---------------------------------------------------------\n",
    "# Load trained chairs modellocal_path\n",
    "model_path = main_path + 'src/train_results/original/chairs.pt'\n",
    "\n",
    "# You can also try loading other examples (e.g. 'chair1.png')\n",
    "img_path  = main_path + 'src/imgs/example-data/chair4.png'\n",
    "\n",
    "scene, model, original_img, rendered_img = render_image(model_path, img_path)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Transform and render image \n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Rotate scene\n",
    "rotations = transformation_dict(azimuth = transformations['azimuth'], elevation = transformations['elevation'])\n",
    "roto_scene = transform_scene(model, scene,init_camera_pos, rotations)\n",
    "\n",
    "\n",
    "# Translate Scene\n",
    "translations = transformation_dict(x = transformations['xyz'][0], \n",
    "                                   y = transformations['xyz'][1], \n",
    "                                   z = transformations['xyz'][2])\n",
    "trans_scene = transform_scene(model, scene, init_camera_pos, translations)\n",
    "\n",
    "# Roto-Translate Scene\n",
    "roto_trans_scene = transform_scene(model, scene, init_camera_pos, transformations)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Loading and plotting the original image\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "\n",
    "images = [original_img, rendered_img, roto_scene, trans_scene, roto_trans_scene]\n",
    "keys = ['Original Image', 'Rendered Image', 'Rotated Image', 'Translated Image', 'Roto-Translated Image']\n",
    "\n",
    "# Plot without transformations\n",
    "fig = display_transformations(images[:2], keys[:2], fsize = (9.8, 3))    \n",
    "fig.suptitle('Demo of Original Rotation Model', size = 15)\n",
    "fig.subplots_adjust(top=0.8)\n",
    "\n",
    "# Plot with transformations\n",
    "fig = display_transformations(images[2:], keys[2:], fsize = (7, 3))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UOqKmSe90M2z"
   },
   "source": [
    "## 2. Response \n",
    "\n",
    "Much of the success of Deep Learning can be attributed to effective representation learning. Such representations do not need to be humanly interpretable, but can also be abstract. The original authors proposed an implicit 3D representation of the scene, instead of an explicit 3D representation such as mesh-grids or point clouds. By removing the need for an explicit 3D representation, they developed a model that requires no 3D supervision. It only requires 2D images with the corresponding rotation angle of the camera, that was used between these images. Their model can generate a novel view from a single image. The qualitative results of their model’s performance motivated us to extent their research.\n",
    "\n",
    "In the original paper the authors used 3D rotations to generate novel views, meaning that they rotate a camera on a sphere around the scene. 3D rotations do not act transitively on 3D space. Therefore, we proposed to extend their model to roto-translations, with the intermediate proof-of-concept step of using translations only. The objective was to obtain a model that can generate a novel view for any camera position in 3D space, within a reasonable range of movement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20zq9asv0nkE"
   },
   "source": [
    "## 3. Novel Contribution\n",
    "\n",
    "In this section we describe the novel contributions of our research.\n",
    "\n",
    "- We introduce a method to generate training data for the equivariant neural rendering models (section 3.1).\n",
    "\n",
    "- We show that the original model, which was solely pretrained on rotations, can already generate plausible translations along the axes running parallel to the image plane (i.e. orthogonal to the line of sight) through inductive bias (section 3.2).\n",
    "\n",
    "- We introduce a model that has been trained on translations and a model that has been trained on roto-translations (section 3.3). This part constitutes the main contribution of our research.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ivUZLqx1LxI"
   },
   "source": [
    "### 3.1 Datasets\n",
    "\n",
    "The authors present datasets consisting of rotational transformations. However, they do not provide instructions or tools for further data generation. To address this limitation we developed a new pipeline using blender for producing images of 3D-models under rotations, translations and roto-translations. Our pipeline can be used to increase the size of the training data, or to extend training data to new transformation groups.\n",
    "\n",
    "The following section demonstrates the practical application of our pipeline for data production, by demonstrating how to use blender to generate new training data containing roto-translations.\n",
    "\n",
    "#### 3.1.1  Demonstration: populating datasets for the ISO(3)-group using Blender \n",
    "Similar to Dupont et al., we perform experiments on the [ShapeNet Core](https://shapenet.org/download/shapenetcore)-Chairs benchmark. It is worth noting that the objects included in the ShapeNetCore dataset are already normalized and consistently aligned. However, the subsequent pipeline can be adapted to accommodate any 3D-object data that is processable by Blender. Here follows a brief demonstration of how data can be constructed using Blender 3.5.1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the subsequent cells once to install Blender with wget**\n",
    "\n",
    "_NB! Installation assumes Linux-based OS._\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Install / Load wget \"\"\"\n",
    "%pip install wget\n",
    "import wget\n",
    "\n",
    "\"\"\" Install blender \"\"\"\n",
    "# Download blender 3.5.1\n",
    "!wget https://ftp.nluug.nl/pub/graphics/blender/release/Blender3.5/blender-3.5.1-linux-x64.tar.xz\n",
    "\n",
    "# Unpack \n",
    "!tar -xvf blender-3.5.1-linux-x64.tar.xz\n",
    "!rm {local_path}/blender-3.5.1-linux-x64.tar.xz\n",
    "\n",
    "# Move and rename for shorter commands\n",
    "!mv {local_path}/blender-3.5.1-linux-x64 {main_path}src/enr/data/demo/blender\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Run render demo_\n",
    "\n",
    "In order to build a roto-translational dataset pass both command line arguments: _--rotation --translation_ . To build a dataset composed only of rotations or translations pass only the singular argument respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Run Demo\"\"\"\n",
    "!{main_path}src/enr/data/demo/blender/blender -b --python {main_path}src/enr/data/demo/render_blender.py -- --scene_name data --rotation --translation --scene_folder src/enr/data/demo/data/model_1 --local_path {main_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Display demonstration of roto-translation dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Display random sample outputs \"\"\"\n",
    "# Load 3 random images from output directory\n",
    "path = main_path + \"src/enr/data/demo/output/rot_trans_dataset/data/\"\n",
    "random_file = [random.choice(os.listdir(path)) for img in range(3)]\n",
    "images = [mpimg.imread(path + image) for image in random_file]\n",
    "\n",
    "# Plot sampleset\n",
    "fig, axs = plt.subplots(1, 3, figsize=(7, 3))\n",
    "for ax, id in zip(axs, range(3)):\n",
    "    ax.imshow(images[id])\n",
    "plt.title('Demonstration of dataset production: Roto-translations of chair model\\n', loc='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyEyGTfZDv51"
   },
   "source": [
    "#### 3.1.2 Populating new datasets\n",
    "\n",
    "We use the afformention pipeline to build 3 new datasets: \n",
    "\n",
    "   * _Rotations_: used to reproduce the results presented by Dupont et al.\n",
    "   * _Translations_: used to train a model with higher capacity for translation invariance.\n",
    "   * _Roto-translations_: used to train a roto-translational invariant model.\n",
    "    \n",
    "We downscale the datasets in order to reduce the computational costs of training the new models. For all three datasets we use the partitioning described in table 2. \n",
    "\n",
    "\n",
    "|   | **# Scenes**  |  **# Images per scene** | **Resolution**  | **# datapoints**  |\n",
    "|---|---|---|---|---|\n",
    "| Train  | 2306  |  50 | 64 x 64  |  115300 |\n",
    "| Validataion  | 331  | 50  |  64 x 64 | 16550  |\n",
    "\n",
    "Table 2: _Partition of new datasets_\n",
    "\n",
    "\n",
    "| **Hyperparameter**  | **R**  |  **X** | **Y**  | **Z**  | **Resolution** |\n",
    "|---|---|---|---|---|---|\n",
    "|   | 1.5  | [-0.4, 0.4]  | [-0.3, 0.5]  | [-0.4, 0.4] | 64 x 64|\n",
    "\n",
    "Table 3: _Hyperparameters used when populating the new dataset._\n",
    "\n",
    "We construct the datasets by sampling poses from various views. In case of rotations the camera is placed on a sphere with a radius **R**. For each view, a value between 0 and $2\\pi$ is uniformly sampled for the elevation and azimuth angle of the camera and rotated accordingly. In case of translations, for each view, a value is uniformly sampled from a range of **X**, **Y** and **Z** locations of the chair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Translations through inductive bias\n",
    "\n",
    "Through inductive bias, (reasonably small) translations, which are orthogonal to the line of sight, already work on the model that has only been trained on rotations. This is due to the fact that the model uses a CNN architecture, which is translationally equivariant along the image plane. Still, it seems interesting that the model does not seem to encode any information from the outside of the object to produce a good estimation.\n",
    "\n",
    "*** More investigation necessary here (check if zero background is mapped to zero in 3D representation, check if translation introduces zero-padding in 3D presentation, check if zeros in 3D representation are mapped to zeros in image) ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translations only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = transformation_dict(y = .3, z = .3)\n",
    "trans_scene = transform_scene(model, scene, init_camera_pos, translations)\n",
    "\n",
    "imgs = [original_img, trans_scene]\n",
    "keys = ['original image', 'translated image']\n",
    "\n",
    "fig = display_transformations(imgs, keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Roto-translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roto_translations = transformation_dict(azimuth = 30., elevation = 30., y = .3, z = .3)\n",
    "roto_trans_scene = transform_scene(model, scene, init_camera_pos, roto_translations)\n",
    "\n",
    "imgs = [original_img, roto_trans_scene]\n",
    "keys = ['original image', 'Roto-translated image']\n",
    "\n",
    "fig = display_transformations(imgs, keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Nonetheless, translations along the line of sight do not work out-of-the-box and require explicit training. The reason for that is that the equivariant neural rendering model considers the depth dimension via incorporating its information into the channels of the CNN. More concrete, the model uses the following code:\n",
    "\n",
    "```python\n",
    "# Reshape 3D -> 2D\n",
    "reshaped = inputs.view(batch_size, channels * depth, height, width)\n",
    "```\n",
    "\n",
    "Furthermore, due to the central positioning of the objects in the images, the model has problems rendering scenes that extent to the image boundaries. Therefore, we trained our model on translations first, before moving on to roto-translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roto_translations = transformation_dict(x = .5)\n",
    "roto_trans_scene = transform_scene(model, scene,init_camera_pos, roto_translations)\n",
    "\n",
    "imgs = [original_img, roto_trans_scene]\n",
    "keys = ['original image', 'Translated image (x-axis)']\n",
    "\n",
    "fig = display_transformations(imgs, keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Extending the model\n",
    "\n",
    "#### 3.3.0 Reproducing rotations with our dataset\n",
    "![image](../src/train_results/2023-05-10_12-31_roto_lr2e-4/rotations.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\" DEMO: We are currently downsampling the image but lose quite a bit of the image quality doing this\n",
    "\"\"\"\n",
    "\n",
    "# Load trained chairs modellocal_path\n",
    "model_path = main_path + 'src/train_results/2023-05-10_12-31_roto_lr2e-4/best_model.pt'\n",
    "\n",
    "# You can also try loading other examples (e.g. 'chair1.png')\n",
    "img_path  = '/imgs/example-data/chair3_small.png'\n",
    "\n",
    "scene, model, original_img, rendered_img = render_image(model_path, img_path, downsample = False)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Transform and render image \n",
    "# ---------------------------------------------------------\n",
    "transformations = transformation_dict(30., 0., 0., -.2, 0.)\n",
    "\n",
    "# Rotate scene\n",
    "rotations = transformation_dict(azimuth = transformations['azimuth'], elevation = transformations['elevation'])\n",
    "roto_scene = transform_scene(model, scene,init_camera_pos, rotations)\n",
    "\n",
    "\n",
    "# Translate Scene\n",
    "translations = transformation_dict(x = transformations['xyz'][0], \n",
    "                                   y = transformations['xyz'][1], \n",
    "                                   z = transformations['xyz'][2])\n",
    "trans_scene = transform_scene(model, scene,init_camera_pos, translations)\n",
    "\n",
    "# Roto-Translate Scene\n",
    "roto_trans_scene = transform_scene(model, scene,init_camera_pos, transformations)\n",
    "\n",
    "\n",
    "images = [original_img, rendered_img, roto_scene, trans_scene, roto_trans_scene]\n",
    "keys = ['Original Image', 'Rendered Image', 'Rotated Image', 'Translated Image', 'Roto-translated Image']\n",
    "\n",
    "# Plot without transformations\n",
    "fig = display_transformations(images[:2], keys[:2], fsize = (9.8, 3))    \n",
    "fig.suptitle('Demo of Our Rotation Model', size = 15)\n",
    "fig.subplots_adjust(top=0.8)\n",
    "\n",
    "# Plot with transformations\n",
    "fig = display_transformations(images[2:], keys[2:], fsize = (7, 3))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 Translations\n",
    "\n",
    "As shown in section 3.2, the original model, which was pretrained on rotation data only, can already produce valid shifts for short distances along the image plane but not along the line of sight. We produced a translation dataset for the model to learn how to deal with more extensive shifts of the camera, as well as learning how to deal with shifts of any size along the line of sight. Because the depth dimension is encoded within the channels of the convolution layers, it is plausible that the model can learn to make use of this information to produce a zoom-like effect, similar to which one would expect when moving the camera closer to the object.\n",
    "\n",
    "Since we can only cover a finite range of translations, we limit the range of shift to -0.5 and 0.5 for all directions. This value was chosen because it still leaves the chair mostly in the image and hence, still contains useful information for training and inference.\n",
    "\n",
    "*** Show the new model and present the results ***\n",
    "**Demo in the making**\n",
    "![image](figs/results/translations.gif)\n",
    "\n",
    "\n",
    "*** Depending on how our experiments go, we might need to talk about model architecture and more inductive biases here ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" DEMO: We are currently downsampling the image but lose quite a bit of the image quality doing this\n",
    "\"\"\"\n",
    "\n",
    "# Load trained chairs modellocal_path\n",
    "model_path = main_path + 'src/train_results/2023-05-14_16-54_trans_lr2e-4/best_model.pt'\n",
    "\n",
    "# You can also try loading other examples (e.g. 'chair1.png')\n",
    "img_path  = '/imgs/example-data/chair4_small.png'\n",
    "\n",
    "scene, model, original_img, rendered_img = render_image(model_path, img_path, downsample = False)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Transform and render image \n",
    "# ---------------------------------------------------------\n",
    "transformations = transformation_dict(20., 0., 0, .1, .1)\n",
    "\n",
    "# Rotate scene\n",
    "rotations = transformation_dict(azimuth = transformations['azimuth'], elevation = transformations['elevation'])\n",
    "roto_scene = transform_scene(model, scene, init_camera_pos, rotations)\n",
    "\n",
    "\n",
    "# Translate Scene\n",
    "translations = transformation_dict(x = transformations['xyz'][0], \n",
    "                                   y = transformations['xyz'][1], \n",
    "                                   z = transformations['xyz'][2])\n",
    "trans_scene = transform_scene(model, scene, init_camera_pos, translations)\n",
    "\n",
    "# Roto-Translate Scene\n",
    "roto_trans_scene = transform_scene(model, scene,init_camera_pos, transformations)\n",
    "\n",
    "\n",
    "images = [original_img, rendered_img, roto_scene, trans_scene, roto_trans_scene]\n",
    "keys = ['Original Image', 'Rendered Image', 'Rotated Image', 'Translated Image', 'Roto-translated Image']\n",
    "\n",
    "# Plot without transformations\n",
    "fig = display_transformations(images[:2], keys[:2], fsize = (9.8, 3))    \n",
    "fig.suptitle('Demo of Our Translation Model', size = 15)\n",
    "fig.subplots_adjust(top=0.8)\n",
    "\n",
    "# Plot with transformations\n",
    "fig = display_transformations(images[2:], keys[2:], fsize = (7, 3))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Roto-Translations\n",
    "\n",
    "The motivation for our research was to extend the equivariant neural rendering model to be able to produce novel views for any camera position in 3D space. Roto-translations fulfill this requirement. More specifically, we can cover all 3D positions by limiting the azimuth angle to a range of -180 to 180 degrees and the elevation angle to a range of -90 to 90 degrees. The latter avoids a flipping of the camera which causes problems with the compatibility of translations in Blender vs translations in the model. Nonetheless, by being able to rotate the camera along the azimuth, we can still cover all necessary views. For translations, we can obviously only cover a finite distances, therefore we chose the same range as in section 3.2 (-0.5 to 0.5 for all directions).\n",
    "\n",
    "*** Show the new model and present the results ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCIdBAQM0wnm"
   },
   "source": [
    "## 4. Conclusion\n",
    "\n",
    "- Some preliminary results (working model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INMCM8Rn031G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4Ykt1aL1Al9"
   },
   "source": [
    "## 5. Contributions \n",
    "\n",
    "Close the notebook with a description of the each students' contribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_MSYFssb1IAm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "6d30045d674a486ab5c434de7ddbe5be62df7fb1ca9297a17bd34125d3fa56d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
