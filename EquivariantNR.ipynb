{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ewRfFs307zB"
      },
      "source": [
        "# Equivariant Neural Rendering"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HTzvhAKd10E_"
      },
      "source": [
        "## Imports\n",
        "- Import git modules & stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dak5naPA3UPI"
      },
      "outputs": [],
      "source": [
        "import random, os, sys\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import image as mpimg\n",
        "%matplotlib inline\n",
        "import imageio\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "\n",
        "sys.path.append('/content/equiv-neural-rendering/')\n",
        "from models.neural_renderer import *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLsHkFYj5s3O"
      },
      "source": [
        "### Loading and plotting the original image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "TSHHexq747BX",
        "outputId": "12111104-4b78-4c64-8e8c-38b22cf117bf"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def plot_img_tensor(img, nrow=4):\n",
        "    \"\"\"Helper function to plot image tensors.\n",
        "    \n",
        "    Args:\n",
        "        img (torch.Tensor): Image or batch of images of shape \n",
        "            (batch_size, channels, height, width).\n",
        "    \"\"\"\n",
        "    img_grid = torchvision.utils.make_grid(img, nrow=nrow)\n",
        "    plt.imshow(img_grid.cpu().numpy().transpose(1, 2, 0))\n",
        "\n",
        "# Load trained chairs model\n",
        "model = load_model('/content/equiv-neural-rendering/trained-models/chairs.pt').to(device)\n",
        "\n",
        "# You can also try loading other examples (e.g. 'chair1.png')\n",
        "img = imageio.imread('/content/equiv-neural-rendering/imgs/example-data/chair4.png')\n",
        "# Visualize image\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SND8hZzMBoF4"
      },
      "source": [
        "### Rendering the scene reprensentation without rotation and translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "PpKDGLKQ5NMz",
        "outputId": "0fc03c07-9af7-4d91-c860-28ae50f80c01"
      },
      "outputs": [],
      "source": [
        "# Convert image to tensor and add batch dimension\n",
        "img_source = ToTensor()(img)\n",
        "img_source = img_source.unsqueeze(0).to(device)\n",
        "\n",
        "# Infer scene representation\n",
        "scene = model.inverse_render(img_source)\n",
        "\n",
        "# We can render the scene representation without rotating it\n",
        "rendered = model.render(scene)\n",
        "\n",
        "plot_img_tensor(rendered.detach())\n",
        "\n",
        "org = rendered.detach().clone()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7aE0r36CSer"
      },
      "source": [
        "### Rotating and translating the scene reprensentation and rendering a novel view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "zdzK7lHL5YRf",
        "outputId": "eb807b13-69bf-4b9f-c61a-4228e2a95b1b"
      },
      "outputs": [],
      "source": [
        "# As a rotation matrix can feel a little abstract, we can also reason in terms of \n",
        "# camera azimuth and elevation. The initial coordinate at which the source image\n",
        "# is observed is given by the following azimuth and elevation. Note that these\n",
        "# are not necessary to generate novel views (as shown above), we just use them \n",
        "# for convenience to generate rotation matrices\n",
        "azimuth_source = torch.Tensor([0.]).to(device)\n",
        "elevation_source = torch.Tensor([0.]).to(device)\n",
        "translations_source = torch.Tensor([0., 0., 0.]).to(device)\n",
        "\n",
        "# You can set these to any value you like!\n",
        "# Positive (negative) values correspond to moving camera to the right (left)\n",
        "azimuth_shift = torch.Tensor([0.]).to(device)  \n",
        "# Positive (negative) values correspond to moving camera up (down)\n",
        "elevation_shift = torch.Tensor([0.]).to(device)\n",
        "# Translation values\n",
        "translations_shift = torch.Tensor([0., -0.5, 0.]).to(device)\n",
        "\n",
        "azimuth_target = azimuth_source + azimuth_shift\n",
        "elevation_target = elevation_source + elevation_shift\n",
        "translations_target = translations_source + translations_shift\n",
        "\n",
        "# Rotate scene to match target camera angle\n",
        "rotated_scene = model.rotate_source_to_target(\n",
        "    scene, \n",
        "    azimuth_source, elevation_source, translations_source,\n",
        "    azimuth_target, elevation_target, translations_target\n",
        ")\n",
        "\n",
        "# Render rotated scene\n",
        "rendered = model.render(rotated_scene)\n",
        "\n",
        "plot_img_tensor(rendered.detach())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBstX2KVztat"
      },
      "source": [
        "## 1. Introduction\n",
        "\n",
        "*analysis of key components*\n",
        "\n",
        "Part 1: Intro\n",
        "- brief introduction on paper:\n",
        "The paper from Dupont et al. introduces an approach to render 2D images into implicit, equivariant 3D representations. The authors argue that the scene representations need not be explicit, as long as the transformations to it occur in an equivariant manner. Their model is trained on a dataset of rotation symmetries, learning to produce novel views from a single image of a scene.\n",
        "\n",
        "- brief motivation of paper (equivariant representations)\n",
        "  - Implicit representations & View synthesis (Figure 2)\n",
        "- define key goal: Learning equivariant scene representations from data\n",
        "\n",
        "  \n",
        "Part 2: Methodology\n",
        "- Model design\n",
        "- Transformations \n",
        "- equivariance, loss-definition (figure 4/5)\n",
        "\n",
        "Part 3: Experiments of paper\n",
        "\n",
        "\n",
        "Part 4: Datasets\n",
        "- Their dataset\n",
        "- Our focus (building new one)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-i1ddTWz4gi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOqKmSe90M2z"
      },
      "source": [
        "## 2. Response \n",
        "\n",
        "Much of the success of Deep Learning can be attributed to effective representation learning. Such representations do not need to be humanly interpretable, but can also be abstract. The original authors proposed an implicit 3D representation of the scene, instead of an explicit 3D representation such as mesh-grids or point clouds. By removing the need for an explicit 3D representation, they developed a model that requires no 3D supervision. It only requires 2D images with the corresponding rotation angle of the camera, that was used between these images. Their model can generate a novel view from a single image. The qualitative results of their modelâ€™s performance motivated us to extent their research.\n",
        "\n",
        "In the original paper the authors used 3D rotations to generate novel views, meaning that they rotate a camera on a sphere around the scene. 3D rotations do not act transitively on 3D space. Therefore, we proposed to extend their model to roto-translations, with the intermediate proof-of-concept step of using translations only. The objective was to obtain a model that can generate a novel view for any camera position in 3D space, within a reasonable range of movement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qw2905eI0nXy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20zq9asv0nkE"
      },
      "source": [
        "## 3. Novel Contribution\n",
        "- Describe your novel contribution.\n",
        "* Methodology/theory for translation & rototranslations\n",
        "  - justify group representation (homogeneous coords for translation and order of matrix multiplcation. \n",
        "\n",
        "- Support your contribution with actual code and experiments (hence the colab format!)\n",
        "  - Demotime\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ivUZLqx1LxI"
      },
      "source": [
        "### 3.1 Datasets\n",
        "\n",
        "The authors present datasets consisting of rotational transformations. However, they do not provide instructions or tools for further data generation. To address this limitation we developed a new pipeline using blender for producing images of 3D-models under rotations, translations and roto-translations. Our pipeline can be used to increase the size of the training data, or to extend training data to new transformation groups.\n",
        "\n",
        "The following section demonstrates the practical application of our pipeline for data production, enabling the generation of new training data for training translation and roto-translational invariant rendering models.\n",
        "\n",
        "#### 3.1.1 Selecting 3D models\n",
        "Similar to the authors, we perform experiments on ShapeNet benchmark. In particular, we download the [ShapeNet Core](https://shapenet.org/download/shapenetcore) subset. It is worth noting that the objects included in the ShapeNetCore dataset are already normalized and consistently aligned. From this subset we extract 2637 models.\n",
        "\n",
        "#### 3.1.2 Build dataset with blender \n",
        "\n",
        "The subsequent pipeline can be adapted to accommodate any 3D-object data that is processable by Blender. Here follows a brief demonstration of how the pipeline can be used using blender 2.8. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c74Ikzd0wSG"
      },
      "outputs": [],
      "source": [
        "\"\"\" Detect local path \"\"\"\n",
        "local_path = !pwd\n",
        "local_path = local_path[0]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_Run the subsequent cells once to install Blender with wget_\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" Install / Load wget \"\"\"\n",
        "%pip install wget\n",
        "import wget\n",
        "\n",
        "\"\"\" Install blender \"\"\"\n",
        "# Download blender 3.5.1\n",
        "!wget https://ftp.nluug.nl/pub/graphics/blender/release/Blender3.5/blender-3.5.1-linux-x64.tar.xz\n",
        "\n",
        "# Unpack \n",
        "!tar -xvf blender-3.5.1-linux-x64.tar.xz\n",
        "!rm {local_path}/blender-3.5.1-linux-x64.tar.xz\n",
        "\n",
        "# Move and rename for shorter commands\n",
        "!mv {local_path}/blender-3.5.1-linux-x64 {local_path}/data_prep/demo/blender\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_Run render demo_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" Run Demo\"\"\"\n",
        "!{local_path}/data_prep/demo/blender/blender -b --python data_prep/demo/render_blender.py -- --scene_name data --rotation --translation --scene_folder /data_prep/demo/data/model_1 --local_path {local_path}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_Display visual demonstration of roto-translation dataset_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" Display random sample outputs \"\"\"\n",
        "# Load 3 random images from output directory\n",
        "path = local_path + \"/data_prep/demo/output/rot_trans_dataset/data/\"\n",
        "random_file = [random.choice(os.listdir(path)) for img in range(3)]\n",
        "images = [mpimg.imread(path + image) for image in random_file]\n",
        "\n",
        "# Plot sampleset\n",
        "fig, axs = plt.subplots(1, 3, figsize=(10, 3))\n",
        "for ax, id in zip(axs, range(3)):\n",
        "    ax.imshow(images[id])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyEyGTfZDv51"
      },
      "source": [
        "\n",
        "\n",
        "#### 3.1.3 New datasets\n",
        "\n",
        "We first reproduce a rotation dataset which we train a model on to verify the reproducibility of the authors orginial results.\n",
        "\n",
        "Subsequently, we produce two new datasets incorporating translations and roto-translations. \n",
        "\n",
        "|   | *# Scenes*  |  *# Images per scene* | Resolution  | *# datapoints*  |\n",
        "|---|---|---|---|---|\n",
        "| Train  | 2306  |  50 | 64 x 64  |  115300 |\n",
        "| Validataion  | 331  | 50  |  64 x 64 | 16550  |\n",
        "\n",
        "The rotations are sampled uniformly on a sphere with radius 1.5\n",
        "\n",
        "We generate 50 images per object by applying transformations rotations, translations, and roto-translations.\n",
        "\n",
        "\n",
        "\n",
        "The following section demonstrates the practical application of our pipeline for data production, enabling the generation of new training data for future research purposes.\n",
        "\n",
        "Include:\n",
        "- blender\n",
        "- further applications using framework \n",
        "- demo of production of new datasets "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCIdBAQM0wnm"
      },
      "source": [
        "## 4. Conclusion\n",
        "\n",
        "- Some preliminary results (working model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INMCM8Rn031G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4Ykt1aL1Al9"
      },
      "source": [
        "## 5. Contributions \n",
        "\n",
        "Close the notebook with a description of the each students' contribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MSYFssb1IAm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
