{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import imageio\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def plot_img_tensor(img, nrow=4):\n",
    "    \"\"\"Helper function to plot image tensors.\n",
    "    \n",
    "    Args:\n",
    "        img (torch.Tensor): Image or batch of images of shape \n",
    "            (batch_size, channels, height, width).\n",
    "    \"\"\"\n",
    "    img_grid = torchvision.utils.make_grid(img, nrow=nrow)\n",
    "    plt.imshow(img_grid.cpu().numpy().transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equivariant Neural Rendering\n",
    "\n",
    "This notebook contains examples of how to load a trained Equivariant Neural Renderer and use it to infer scene representations from a single image, as well as rendering novel views of the scene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.neural_renderer import load_model\n",
    "\n",
    "# Load trained chairs model\n",
    "model = load_model('trained-models/chairs.pt').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also try loading other examples (e.g. 'chair1.png')\n",
    "img = imageio.imread('imgs/example-data/chair4.png')\n",
    "# Visualize image\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer and render scene representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Convert image to tensor and add batch dimension\n",
    "img_source = ToTensor()(img)\n",
    "img_source = img_source.unsqueeze(0).to(device)\n",
    "\n",
    "# Infer scene representation\n",
    "scene = model.inverse_render(img_source)\n",
    "\n",
    "# Print scene shape\n",
    "print(\"Scene shape: {}\".format(scene.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can render the scene representation without rotating it\n",
    "rendered = model.render(scene)\n",
    "\n",
    "plot_img_tensor(rendered.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate novel views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize a rotation matrix\n",
    "# rotation_matrix = torch.Tensor(\n",
    "#    [[[ 0.4198, -0.3450, -0.8395],\n",
    "#      [-0.2159,  0.8605, -0.4615],\n",
    "#      [ 0.8816,  0.3749,  0.2867]]]\n",
    "# ).to(device)\n",
    "\n",
    "# # Rotate scene by rotation matrix\n",
    "# rotated_scene = model.rotate(scene, rotation_matrix)\n",
    "\n",
    "# # Render rotated scene\n",
    "# rendered = model.render(rotated_scene)\n",
    "\n",
    "# plot_img_tensor(rendered.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a rotation matrix can feel a little abstract, we can also reason in terms of \n",
    "# camera azimuth and elevation. The initial coordinate at which the source image\n",
    "# is observed is given by the following azimuth and elevation. Note that these\n",
    "# are not necessary to generate novel views (as shown above), we just use them \n",
    "# for convenience to generate rotation matrices\n",
    "azimuth_source = torch.Tensor([42.561195]).to(device)\n",
    "elevation_source = torch.Tensor([23.039995]).to(device)\n",
    "translations_source = torch.Tensor([0., 0., 0.]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set these to any value you like!\n",
    "# Positive (negative) values correspond to moving camera to the right (left)\n",
    "azimuth_shift = torch.Tensor([0.]).to(device)  \n",
    "# Positive (negative) values correspond to moving camera up (down)\n",
    "elevation_shift = torch.Tensor([0.]).to(device)\n",
    "# Translation values\n",
    "translations_shift = torch.Tensor([0., 0.5, 0.]).to(device)\n",
    "\n",
    "azimuth_target = azimuth_source + azimuth_shift\n",
    "elevation_target = elevation_source + elevation_shift\n",
    "translations_target = translations_source + translations_shift\n",
    "\n",
    "# Rotate scene to match target camera angle\n",
    "rotated_scene = model.rotate_source_to_target(\n",
    "    scene, \n",
    "    azimuth_source, elevation_source, translations_source,\n",
    "    azimuth_target, elevation_target, translations_target\n",
    ")\n",
    "\n",
    "# Render rotated scene\n",
    "rendered = model.render(rotated_scene)\n",
    "\n",
    "plot_img_tensor(rendered.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from misc.viz import generate_novel_views\n",
    "\n",
    "# We can also generate several novel views of the same object\n",
    "azimuth_shifts = torch.Tensor([20., -50., 120., 180., -90., 50.]).to(device)\n",
    "elevation_shifts = torch.Tensor([10., -30., 40., -70., 10., 30.]).to(device)\n",
    "\n",
    "# This function expects a single image as input, so remove batch dimension\n",
    "views = generate_novel_views(model, img_source[0], azimuth_source, elevation_source,\n",
    "                             azimuth_shifts, elevation_shifts)\n",
    "\n",
    "plot_img_tensor(views.detach(), nrow=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate novel view animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from misc.utils import full_rotation_angle_sequence, sine_squared_angle_sequence\n",
    "from misc.viz import batch_generate_novel_views, save_img_sequence_as_gif\n",
    "\n",
    "num_frames = 25\n",
    "\n",
    "azimuth_shifts = full_rotation_angle_sequence(num_frames).to(device)\n",
    "elevation_shifts = sine_squared_angle_sequence(num_frames, -10., 20.).to(device)\n",
    "\n",
    "views = batch_generate_novel_views(model, img_source, azimuth_source,\n",
    "                                   elevation_source, azimuth_shifts,\n",
    "                                   elevation_shifts)\n",
    "\n",
    "# Save generated gif (this gif will display in the next cell once it has been saved)\n",
    "save_img_sequence_as_gif(views, 'imgs/novel_views_example.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![chairs-gif](./novel_views_example.gif \"chairs-gif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
